## Purpose

This is copied from `16-keras-cnn-higher-dropout-l2`. Now the second last
layers have double the number of neurons.


## Results

Before, 77.42% test accuracy.

```
Epoch 00700: val_loss did not improve from 0.09115
16992/16992 [==============================] - 1s 74us/step

acc: 75.19%

real    8558,82s
user    12615,26s
sys    1213,58s
```

and

```
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d_1 (Conv2D)            (None, 30, 30, 16)        160       
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 15, 15, 16)        0         
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 13, 13, 16)        2320      
_________________________________________________________________
flatten_1 (Flatten)          (None, 2704)              0         
_________________________________________________________________
dense_1 (Dense)              (None, 256)               692480    
_________________________________________________________________
dropout_1 (Dropout)          (None, 256)               0         
_________________________________________________________________
dense_2 (Dense)              (None, 512)               131584    
_________________________________________________________________
dropout_2 (Dropout)          (None, 512)               0         
_________________________________________________________________
dense_3 (Dense)              (None, 369)               189297    
=================================================================
Total params: 1,015,841
Trainable params: 1,015,841
Non-trainable params: 0
_________________________________________________________________
None
16992/16992 [==============================] - 2s 113us/step

acc: 75.39%
```

=> Overfitting is a problem