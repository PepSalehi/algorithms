## Purpose

This is copied from `16-keras-cnn-higher-dropout-l2`. Now the second last layer relu activation function.


## Results

Before, 77.42% accuracy.

```
Epoch 00700: val_loss did not improve from 0.27946
16992/16992 [==============================] - 1s 77us/step

acc: 76.99%

real    8577,93s
user    12688,50s
sys    1192,16s
```

and

```
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d_1 (Conv2D)            (None, 30, 30, 16)        160       
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 15, 15, 16)        0         
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 13, 13, 16)        2320      
_________________________________________________________________
flatten_1 (Flatten)          (None, 2704)              0         
_________________________________________________________________
dense_1 (Dense)              (None, 128)               346240    
_________________________________________________________________
dropout_1 (Dropout)          (None, 128)               0         
_________________________________________________________________
dense_2 (Dense)              (None, 256)               33024     
_________________________________________________________________
dropout_2 (Dropout)          (None, 256)               0         
_________________________________________________________________
dense_3 (Dense)              (None, 369)               94833     
=================================================================
Total params: 476,577
Trainable params: 476,577
Non-trainable params: 0
_________________________________________________________________
None
16992/16992 [==============================] - 2s 147us/step

acc: 76.32%
```

=> Performance dropped a bit